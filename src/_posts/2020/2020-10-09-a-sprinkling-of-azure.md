---
layout: post
date: 2020-09-30T12:57:47.376Z
title: Replicating Wellcome Collection's digital archive to Azure Blob Storage
tags:
  - wellcome collection
  - digital preservation
  - azure
colors:
  css_light: "#235b99"
  css_dark:  "#82baf3"
summary: How and why we keep copies of Wellcome's digital collections in multiple cloud storage providers.
---
*This article was originally published [on the Wellcome Collection development blog](https://stacks.wellcomecollection.org/a-sprinkling-of-azure-6cef6e150fb2) under a CC BY 4.0 license, and is reposted here in accordance with that license.*

<p>Our cloud storage service is designed to ensure the <a href="https://stacks.wellcomecollection.org/building-wellcome-collections-new-archival-storage-service-3f68ff21927e#5bf4">long-term preservation</a> of our digital collections. As an archive, we have an obligation to ourselves and to our depositors to keep our collections safe. We’ve spent millions of pounds digitising our physical objects, and some of our born-digital and audiovisual material is irreplaceable.</p><p>One way we do this is by storing multiple copies of every file. If one copy were to be corrupted or deleted, we’d still have other copies that we could use to construct the complete archive.</p><p>In the initial iteration of our storage service, we kept two copies of every file in a pair of Amazon S3 buckets. We’ve recently upgraded the storage service to keep a third copy of every file in Azure Blob Storage, and in a different geographic location. In this post, I’m going to explain why this change was important, and how we made it.</p>

<figure>
  {%
    picture
    filename="Azurblau_Pigment.JPG"
    width="750"
    alt="A pile of blue dusty powder"
  %}
  <figcaption>Photo by Marco Almbauer, taken from <a href="https://commons.wikimedia.org/wiki/File:Azurblau_Pigment.JPG">Wikimedia Commons</a>, used under CC BY-SA 3.0.</figcaption></figure>

<h2>Why do we need another storage provider?</h2>

<p>Before we started this work, both copies of our digital collections were kept in two S3 buckets. Both buckets are in the same AWS account.</p><p><strong>Our AWS account is a single point of failure. </strong>Although Amazon invest heavily in reliability and durability — for example, objects in S3 are stored across <a href="https://aws.amazon.com/s3/faqs/#AWS_Regions">multiple, distinct facilities (“Availability Zones”)</a> and should survive the destruction of a single data centre — you can imagine scenarios in which we lose everything in our S3 buckets.</p><p>Amazon could close our account because of a payment issue. Somebody could accidentally run a script that empties both buckets. There could be a systemic issue that affects all content in S3.</p><p><strong>Adding a second provider removes this single point of failure. </strong>All of these scenarios could still happen, but another copy of the data in a separate provider means they would be inconvenient rather than disastrous. There are fewer scenarios that would affect multiple providers simultaneously.</p><h2>Why do we need another geographic location?</h2><p>All of our data in S3 is kept in a single <a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/">AWS region</a>. An AWS region is a collection of data centres in the same geographic area. We use the eu-west-1 region, which means our data is somewhere in Ireland.</p><p><strong>Having all our data in the same geographic location is another single point of failure. </strong>Although our data is spread across multiple data centres, you can imagine manmade or natural disasters in which many data centres are affected at the same time.</p><p><strong>Adding a second geographic location removes this single point of failure. </strong>There are fewer disasters that would affect multiple locations at once, especially if (as our replicas are) the locations are on different landmasses.</p><h2>Which location and provider did we choose?</h2><p>We chose Azure Blob Storage because Wellcome is already using Azure for a number of other services, including Active Directory and SQL Databases — but the account is owned and managed by a different team. That means the account has a different payment setup, different support contacts, and a different authentication system. Even the internal processes for getting access to our AWS and Azure accounts are different.</p><p><strong>There’s very little overlap between our AWS and Azure accounts, which minimises the chance of a systemic failure that affects both. </strong>That overlap includes people. At time of writing, just three people have access to all three replicas, and eventually we’d like to reduce that number. That means there are very few people who could wipe out every copy of the archive (whether accidentally or maliciously.)</p><p>We did look at other cloud storage providers — there are plenty of good choices, all of which would probably have worked similarly well. Azure was the best choice for us, but it may not be for you. <strong>If you have critical data in cloud storage, the important thing is to have a backup in another cloud. Which cloud you use as backup is less important.</strong></p><p>We’re using Azure’s West Europe (Netherlands) region. We chose it because it’s a good distance from Ireland, and roughly equidistant from the Wellcome Collection building. Again, the exact choice isn’t so important — the important thing is that it’s a different geographic location from our data in S3.</p>

---

<h2>How did we implement replication to Azure?</h2><p>We reused <a href="https://stacks.wellcomecollection.org/building-wellcome-collections-new-archival-storage-service-3f68ff21927e#66a9">the design we have for storing files in S3</a>. The unit of storage in our digital archive is <a href="https://tools.ietf.org/html/rfc8493">a BagIt “bag”</a>, and each bag is passed through a series of applications. Each application does one step in the process of storing a bag — unpacking a compressed archive, verifying the bag’s contents, copying the bag to long-term storage, and so on.</p><p>We had to create new versions of our applications that could manage bags in Azure Blob Storage:</p><ul class=""><li>The <em>bag replicator </em>copies a bag from working storage (in S3) to permanent storage (in S3 or Azure). Previously our replicator could only write bags to S3; we had to extend it to write bags into Azure Blob Storage.</li><li>A BagIt “bag” includes a file manifest, with a checksum for every file in the bag. The <em>bag verifier </em>compares the stored bag to the checksums in the manifest, to confirm the bag was written correctly. We had to build a new verifier that could inspect bags in Azure.</li><li>The <em>replica aggregator</em> counts the number of verified copies of a bag, and warns us if a bag doesn’t have enough copies. Previously it only counted two replicas, now it has to count three.</li></ul><p>We built prototypes of these new apps, and then we added them to the pipeline for new bags. That meant that any new bags that were stored were being written to S3 and Azure, but all the existing bags were only stored in S3. We left this running for a few weeks to see how our new code worked with real bags.</p><p>Once we were confident these new apps were working, we replicated all of our existing bags into Azure. That took several weeks, after which our Azure storage account was byte-for-byte identical to our existing S3 buckets. As we were doing it, we kept tweaking the apps to handle unusual bags and improve reliability.</p>

---

<h2>What was hard?</h2><p>There were a number of challenges writing bags to Azure.</p><h2>Getting used to Azure</h2><p>Until now, the Platform team has worked entirely in AWS. Although S3 and Blob Storage are similar (they’re both distributed object stores), they differ in the details, and it took us time to get used to the ins and outs of Azure Blob Storage.</p><p>This is especially important when things go wrong. We have plenty of experience with the edge cases of S3, and the storage service has code to handle and retry unusual errors. We don’t have that experience with Azure, and we had to learn very quickly.</p><h2>Refactoring our code to work with Azure</h2><p>We’ve known we wanted to add another storage provider since the earliest designs of the storage service, and we tried to write our code in a <a href="/2019/sans-io-programming/">sans I/O</a>, provider-agnostic way. For example, the code that parses a BagIt “bag” manifest shouldn’t have to care whether the bag is in S3 or Blob Storage.</p><p>In practice, there were still plenty of cases where our code assumed that it would only ever talk to S3. We had to do a lot of refactoring to remove these assumptions before we could start adding Azure support.</p><p>Although we have no immediate plans to do so, doing this work once should make it easier to add another provider in the future. The hard part was making our code truly generic enough to handle more than one provider; after that, adding additional providers should be simpler.</p>

<h2>The cost of cross-cloud data transfer</h2><p>Copying your data into a public cloud is free — but if you want to get it out again, you have to pay. Copying a gigabyte of data out of S3 costs as much as storing it for nearly <em>four months</em>.</p><p>Our complete data transfer cost was about $10k for our 60TB archive— not eye-wateringly expensive, but enough that we didn’t want to do it more than necessary. (We paid once to replicate everything from S3 to Azure, and once for the verifier running in AWS to read everything from Azure to check we wrote the correct bytes.)</p><p>We took several steps to keep our data transfer costs down, including caching the results of the Azure bag verifier, and <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html">running our own NAT instances</a> in AWS to avoid the bandwidth cost of Amazon’s <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">managed NAT Gateway</a>.</p>

<h2>Reading and writing large files</h2><p>Some of the files we have are fairly large — the largest we’ve seen so far is 166GiB, which is a preservation copy of a digitised video.</p><p>Trying to read or write such a large file in a single request can be risky. You need to hold open the same connection for a long time, and if anything goes wrong (say, a timeout or a dropped connection), you have to start from scratch. It’s generally more reliable to break a large file into smaller pieces, read or write each piece individually, then stitch the pieces back together. Both AWS and Azure encourage this approach and have limits on how much data you can write in a single piece.</p><p>We’ve had to write a lot of code so that we can reliably read and write blobs of all sizes — with our initial prototypes, we saw dropped connections and timeouts when working with larger blobs. Some of this was adapting code <a href="/2019/streaming-large-s3-objects/">we’d already written for S3</a>; some of it was entirely new code for Azure.</p><p>This is an area where the S3 SDK is more mature than Azure. The S3 Java SDK provides the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/transfer/TransferManager.html">TransferManager class</a> for uploading large files, which handles the details of breaking the file into smaller pieces. The Azure Java SDK doesn’t provide a comparable class — we had to write and debug our own code for uploading large files. We’re surely not the first people to need this functionality, and it would be nice if it was provided in the SDK.</p>

---

<h2>Where we are now</h2><p>Every bag in the storage service has three copies: one in Amazon S3, one in Amazon Glacier, one in Azure Blob Storage. When we store new bags, they’re copied to each of these locations.</p><p>This means our digital archive is much more resilient and less likely to suffer catastrophic data loss. This gives us more confidence as we continue to build on it and use it for ever-more data.</p>
